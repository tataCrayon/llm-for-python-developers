# 1 多数据源处理

有文档解析引擎可以参考，比如PP-Structure
这里是自己实现Processor Handler。

# 2 知识库Chunk设计

在最开始的版本中，我使用的是结构化分块，具体是通过递归实现的，评估的结果并不是很理想。

V1:结构化分块（递归:先按标题+内容段落，最后按固定长度） + 窗口重叠

决定更新版本，采用混合策略，引入语义感知分块+基于实体/概念的分块。

V2:V1+语义+实体/概念

力求让每一个分块都能完整地包含有一个问题的可用信息（语义）。

良好的chunk必须保证语义的完整性、结构完整性。

# 3 元数据设计

结合之前实际落地的项目来看，至少要包含以下基础信息：

- 基础信息  
  ID、来源（URL）、创建时间、摘要、作者

其中ID的常见方案为：doc_id_chunk_num（文档ID+块编号）。
多数时候这个块编号整数也够用，UUIDV4或者雪花ID也行。
还可以考虑为ID添加版号信息。

- 额外信息  
  图片|logo、标签、权限控制、语言

需要有业务需求进行设计，但基础信息应当是通用的，先设计着。

当前元数据设计：id, source, file_name, extension, chunk_index, chunk_strategy, chunk_overlap, keywords, context, weight

## 3.1 关键词设计

元数据的关键词刚开始使用的基于词频和位置的yake。

- V1版本 - yake

```python
keyword_list = [kw[0] for kw in kw_extractor.extract_keywords(chunk)]
keywords = ','.join(keyword_list)
```

**问题1**：yake 是一种基于统计的无监督方法，依赖词频和位置，难以捕捉语义信息，可能提取出不相关的关键词。
**问题2**：缺乏上下文理解，导致关键词可能不够代表文档的核心内容。

V2版本 - 引入小模型做chunk语义总结。

做了简单测试，使用Qwen2.5-1.5B-Instruct模型做关键词提取结果和Qwen3-235B-A22B-2507模型效果是一样的。

