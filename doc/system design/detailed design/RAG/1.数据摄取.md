# 1 多数据源处理

一个生产级的RAG需求，应当是需要支持多数据源的数据。  
要支持多数据源，核心在于如何统一处理和表示不同类型的数据，以便后续进行有效的检索和生成。

数据来源不管是网页、数据库、还是文档、API接口。  
都需要做转换处理。  
使用HTML库、转换结构、半结构数据、解析文档的库等等...

然后最终都需要转换成一个方便分块的文本，而这种文本一般被封装成一个Document对象。

比较成熟的可以借鉴的方案有LlamaIndex和LangChain。

- LlamaIndex   
    - Github：https://github.com/run-llama/llama_index
    - 官网：https://docs.llamaindex.ai/en/stable/

设计代码需要注意的是模块化设计与灵活扩展。


# 2 知识库Chunk设计

## 2.1 chunk逻辑

良好的chunk必须保证语义的完整性、结构完整性。

- 初始chunk设计
```python
class MDProcessorConfig:
    """
    Markdown 处理器配置类
    """
    def __init__(
            self,
            chunk_strategy: str = "recursive",  # 分块策略：paragraph, heading, fixed_length ,recursive
            max_chunk_size: int = 500,  # 固定长度分块的最大字符数
            chunk_overlap: int = 50,  # 相邻块之间的重叠字符数
            heading_level: int = 3  # 按标题分块的最大标题级别（例如 h1-h3）
    ):
```

分块策略：递归分块——先按标题+内容段落，最后按固定长度

- 改进设计
标题与内容合并，分块大小提升。
```python
def __init__(  
        self,        chunk_strategy: str = "recursive",  # 分块策略：paragraph, heading, fixed_length ,recursive  
        max_chunk_size: int = 1500,  # 固定长度分块的最大字符数  
        chunk_overlap: int = 200,  # 相邻块之间的重叠字符数  
        heading_level: int = 4,  # 按标题分块的最大标题级别（例如 h1-h4）  
        min_chunk_size: int = 50  # 最小块大小  
):
```


# 3 元数据设计

结合之前实际落地的项目来看，至少要包含以下基础信息：

- 基础信息  
ID、来源（URL）、创建时间、摘要、作者

其中ID的常见方案为：doc_id_chunk_num（文档ID+块编号）。
多数时候这个块编号整数也够用，UUIDV4或者雪花ID也行。
还可以考虑为ID添加版号信息。

- 额外信息  
图片|logo、标签、权限控制、语言

需要有业务需求进行设计，但基础信息应当是通用的，先设计着。

当前元数据设计：id, source, file_name, extension, chunk_index, chunk_strategy, chunk_overlap, keywords, context, weight


## 3.1 关键词设计

- 初始版本 - yake
```python
keyword_list = [kw[0] for kw in kw_extractor.extract_keywords(chunk)]
keywords = ','.join(keyword_list)
```

**问题1**：yake 是一种基于统计的无监督方法，依赖词频和位置，难以捕捉语义信息，可能提取出不相关的关键词。
**问题2**：缺乏上下文理解，导致关键词可能不够代表文档的核心内容。

- 改进方案 TODO
可以使用别的关键词提取算法，如TF-IDF。
这里的方法是引入预训练模型（BERT）。


